{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "037fa939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession # The entry point to programming Spark with the Dataset and DataFrame API\n",
    "\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"Test_DataLake_House\")\\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\\\n",
    "    .config(\"spark.executor.memory\", \"2g\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65199c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession started\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o105.get.\n: java.util.NoSuchElementException: None.get\r\n\tat scala.None$.get(Option.scala:529)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\IPython\\core\\formatters.py:406\u001b[39m, in \u001b[36mBaseFormatter.__call__\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    404\u001b[39m     method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\sql\\session.py:277\u001b[39m, in \u001b[36mSparkSession._repr_html_\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_repr_html_\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[33m        <div>\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[33m            <p><b>SparkSession - \u001b[39m\u001b[38;5;132;01m{catalogImplementation}\u001b[39;00m\u001b[33m</b></p>\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[33m            \u001b[39m\u001b[38;5;132;01m{sc_HTML}\u001b[39;00m\n\u001b[32m    274\u001b[39m \u001b[33m        </div>\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m.format(\n\u001b[32m    276\u001b[39m         catalogImplementation=\u001b[38;5;28mself\u001b[39m.conf.get(\u001b[33m\"\u001b[39m\u001b[33mspark.sql.catalogImplementation\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         sc_HTML=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_repr_html_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\context.py:321\u001b[39m, in \u001b[36mSparkContext._repr_html_\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_repr_html_\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m    307\u001b[39m \u001b[33;43m    <div>\u001b[39;49m\n\u001b[32m    308\u001b[39m \u001b[33;43m        <p><b>SparkContext</b></p>\u001b[39;49m\n\u001b[32m    309\u001b[39m \n\u001b[32m    310\u001b[39m \u001b[33;43m        <p><a href=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{sc.uiWebUrl}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m>Spark UI</a></p>\u001b[39;49m\n\u001b[32m    311\u001b[39m \n\u001b[32m    312\u001b[39m \u001b[33;43m        <dl>\u001b[39;49m\n\u001b[32m    313\u001b[39m \u001b[33;43m          <dt>Version</dt>\u001b[39;49m\n\u001b[32m    314\u001b[39m \u001b[33;43m            <dd><code>v\u001b[39;49m\u001b[38;5;132;43;01m{sc.version}\u001b[39;49;00m\u001b[33;43m</code></dd>\u001b[39;49m\n\u001b[32m    315\u001b[39m \u001b[33;43m          <dt>Master</dt>\u001b[39;49m\n\u001b[32m    316\u001b[39m \u001b[33;43m            <dd><code>\u001b[39;49m\u001b[38;5;132;43;01m{sc.master}\u001b[39;49;00m\u001b[33;43m</code></dd>\u001b[39;49m\n\u001b[32m    317\u001b[39m \u001b[33;43m          <dt>AppName</dt>\u001b[39;49m\n\u001b[32m    318\u001b[39m \u001b[33;43m            <dd><code>\u001b[39;49m\u001b[38;5;132;43;01m{sc.appName}\u001b[39;49;00m\u001b[33;43m</code></dd>\u001b[39;49m\n\u001b[32m    319\u001b[39m \u001b[33;43m        </dl>\u001b[39;49m\n\u001b[32m    320\u001b[39m \u001b[33;43m    </div>\u001b[39;49m\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m \u001b[33;43m    \u001b[39;49m\u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m        \u001b[49m\u001b[43msc\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\context.py:437\u001b[39m, in \u001b[36mSparkContext.uiWebUrl\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34muiWebUrl\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    436\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the URL of the SparkUI instance started by this SparkContext\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsc\u001b[49m\u001b[43m.\u001b[49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43muiWebUrl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1315\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1316\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1317\u001b[39m     args_command +\\\n\u001b[32m   1318\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1320\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1325\u001b[39m     temp_arg._detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\sql\\utils.py:111\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a, **kw):\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j.protocol.Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    113\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o105.get.\n: java.util.NoSuchElementException: None.get\r\n\tat scala.None$.get(Option.scala:529)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x226ffba1d00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"FastSession\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"in-memory\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession started\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60be1b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession started: 3.2.4\n",
      "App name: FastSession\n"
     ]
    }
   ],
   "source": [
    "print(\"SparkSession started:\", spark.version)\n",
    "print(\"App name:\", spark.sparkContext.appName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73eace2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java version \"1.8.0_451\"\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_451-b10)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.451-b10, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fb3245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"FastSession\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"in-memory\") \\\n",
    "    .config(\"spark.ui.enabled\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67abc595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7823d9ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      3\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal[*]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFastSession\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.catalogImplementation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43min-memory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.ui.enabled\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m df = spark.read.csv(path=\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mD:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdata_cleaning_09_04\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m5_22_2025\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mNouran Living 1 New.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\sql\\session.py:477\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    475\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    476\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    480\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\context.py:512\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    510\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    511\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\context.py:198\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    194\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    195\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    196\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    201\u001b[39m         master,\n\u001b[32m    202\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    212\u001b[39m         memory_profiler_cls,\n\u001b[32m    213\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\context.py:432\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    431\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    433\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\java_gateway.py:103\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc.poll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m    106\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mJava gateway process exited before sending its port number\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"FastSession\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"in-memory\") \\\n",
    "    .config(\"spark.ui.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(path=r\"D:\\data_cleaning_09_04\\5_22_2025\\Nouran Living 1 New.csv\")\n",
    "print(df)\n",
    "!python data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5128aaab",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)\r\n\tat py4j.Gateway.invoke(Gateway.java:237)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal[*]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(spark.version)\n\u001b[32m      5\u001b[39m spark.stop()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\sql\\session.py:480\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    477\u001b[39m     sc = SparkContext.getOrCreate(sparkConf)\n\u001b[32m    478\u001b[39m     \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    479\u001b[39m     \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     session = \u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    482\u001b[39m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m    483\u001b[39m         \u001b[38;5;28mgetattr\u001b[39m(session._jvm, \u001b[33m\"\u001b[39m\u001b[33mSparkSession$\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mMODULE$\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    484\u001b[39m     ).applyModifiableSettings(session._jsparkSession, \u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\sql\\session.py:525\u001b[39m, in \u001b[36mSparkSession.__init__\u001b[39m\u001b[34m(self, sparkContext, jsparkSession, options)\u001b[39m\n\u001b[32m    521\u001b[39m         \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._jvm, \u001b[33m\"\u001b[39m\u001b[33mSparkSession$\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mMODULE$\u001b[39m\u001b[33m\"\u001b[39m).applyModifiableSettings(\n\u001b[32m    522\u001b[39m             jsparkSession, options\n\u001b[32m    523\u001b[39m         )\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m         jsparkSession = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsc\u001b[49m\u001b[43m.\u001b[49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    527\u001b[39m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._jvm, \u001b[33m\"\u001b[39m\u001b[33mSparkSession$\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mMODULE$\u001b[39m\u001b[33m\"\u001b[39m).applyModifiableSettings(\n\u001b[32m    528\u001b[39m         jsparkSession, options\n\u001b[32m    529\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\py4j\\java_gateway.py:1627\u001b[39m, in \u001b[36mJavaClass.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1621\u001b[39m command = proto.CONSTRUCTOR_COMMAND_NAME +\\\n\u001b[32m   1622\u001b[39m     \u001b[38;5;28mself\u001b[39m._command_header +\\\n\u001b[32m   1623\u001b[39m     args_command +\\\n\u001b[32m   1624\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1626\u001b[39m answer = \u001b[38;5;28mself\u001b[39m._gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1627\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1631\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\py4j\\protocol.py:331\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    335\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    336\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    337\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name))\n",
      "\u001b[31mPy4JError\u001b[39m: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)\r\n\tat py4j.Gateway.invoke(Gateway.java:237)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()\n",
    "print(spark.version)\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5e841ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\serializers.py\", line 437, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"c:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 563, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 653, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 526, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 507, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py\", line 236, in _extract_code_globals\n",
      "    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\serializers.py:437\u001b[39m, in \u001b[36mCloudPickleSerializer.dumps\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m pickle.PickleError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:73\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, protocol, buffer_callback)\u001b[39m\n\u001b[32m     70\u001b[39m cp = CloudPickler(\n\u001b[32m     71\u001b[39m     file, protocol=protocol, buffer_callback=buffer_callback\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43mcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m file.getvalue()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:563\u001b[39m, in \u001b[36mCloudPickler.dump\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:653\u001b[39m, in \u001b[36mCloudPickler.reducer_override\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types.FunctionType):\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;66;03m# distpatch_table\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:526\u001b[39m, in \u001b[36mCloudPickler._function_reduce\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dynamic_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:507\u001b[39m, in \u001b[36mCloudPickler._dynamic_function_reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m    506\u001b[39m newargs = \u001b[38;5;28mself\u001b[39m._function_getnewargs(func)\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m state = \u001b[43m_function_getstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (types.FunctionType, newargs, state, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    509\u001b[39m         _function_setstate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:157\u001b[39m, in \u001b[36m_function_getstate\u001b[39m\u001b[34m(func)\u001b[39m\n\u001b[32m    146\u001b[39m slotstate = {\n\u001b[32m    147\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__name__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__name__\u001b[39m,\n\u001b[32m    148\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__qualname__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__qualname__\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    154\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__closure__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__closure__\u001b[39m,\n\u001b[32m    155\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m f_globals_ref = \u001b[43m_extract_code_globals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__code__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m f_globals = {k: func.\u001b[34m__globals__\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f_globals_ref \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m\n\u001b[32m    159\u001b[39m              func.\u001b[34m__globals__\u001b[39m}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py:236\u001b[39m, in \u001b[36m_extract_code_globals\u001b[39m\u001b[34m(co)\u001b[39m\n\u001b[32m    235\u001b[39m names = co.co_names\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m out_names = {\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43moparg\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _, oparg \u001b[38;5;129;01min\u001b[39;00m _walk_global_ops(co)}\n\u001b[32m    238\u001b[39m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[38;5;66;03m# syntax generates a constant code object corresonding to the one\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPicklingError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m date, datetime\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Row\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstring1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatetime\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstring2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatetime\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstring3\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatetime\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ma long, b double, c string, d date, e timestamp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\sql\\session.py:675\u001b[39m, in \u001b[36mSparkSession.createDataFrame\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m    671\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas.DataFrame):\n\u001b[32m    672\u001b[39m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[32m    673\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m).createDataFrame(\n\u001b[32m    674\u001b[39m         data, schema, samplingRatio, verifySchema)\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\sql\\session.py:701\u001b[39m, in \u001b[36mSparkSession._create_dataframe\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m    699\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    700\u001b[39m     rdd, schema = \u001b[38;5;28mself\u001b[39m._createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m jrdd = \u001b[38;5;28mself\u001b[39m._jvm.SerDeUtil.toJavaArray(\u001b[43mrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_to_java_object_rdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    702\u001b[39m jdf = \u001b[38;5;28mself\u001b[39m._jsparkSession.applySchemaToPythonRDD(jrdd.rdd(), schema.json())\n\u001b[32m    703\u001b[39m df = DataFrame(jdf, \u001b[38;5;28mself\u001b[39m._wrapped)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\rdd.py:2620\u001b[39m, in \u001b[36mRDD._to_java_object_rdd\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2614\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\" Return a JavaRDD of Object by unpickling\u001b[39;00m\n\u001b[32m   2615\u001b[39m \n\u001b[32m   2616\u001b[39m \u001b[33;03mIt will convert each Python object into Java object by Pyrolite, whenever the\u001b[39;00m\n\u001b[32m   2617\u001b[39m \u001b[33;03mRDD is serialized in batch or not.\u001b[39;00m\n\u001b[32m   2618\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2619\u001b[39m rdd = \u001b[38;5;28mself\u001b[39m._pickled()\n\u001b[32m-> \u001b[39m\u001b[32m2620\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm.SerDeUtil.pythonToJava(\u001b[43mrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\rdd.py:2951\u001b[39m, in \u001b[36mPipelinedRDD._jrdd\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2948\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2949\u001b[39m     profiler = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2951\u001b[39m wrapped_func = \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2952\u001b[39m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2953\u001b[39m python_rdd = \u001b[38;5;28mself\u001b[39m.ctx._jvm.PythonRDD(\u001b[38;5;28mself\u001b[39m._prev_jrdd.rdd(), wrapped_func,\n\u001b[32m   2954\u001b[39m                                      \u001b[38;5;28mself\u001b[39m.preservesPartitioning, \u001b[38;5;28mself\u001b[39m.is_barrier)\n\u001b[32m   2955\u001b[39m \u001b[38;5;28mself\u001b[39m._jrdd_val = python_rdd.asJavaRDD()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\rdd.py:2830\u001b[39m, in \u001b[36m_wrap_function\u001b[39m\u001b[34m(sc, func, deserializer, serializer, profiler)\u001b[39m\n\u001b[32m   2828\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[33m\"\u001b[39m\u001b[33mserializer should not be empty\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2829\u001b[39m command = (func, profiler, deserializer, serializer)\n\u001b[32m-> \u001b[39m\u001b[32m2830\u001b[39m pickled_command, broadcast_vars, env, includes = \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2831\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sc._jvm.PythonFunction(\u001b[38;5;28mbytearray\u001b[39m(pickled_command), env, includes, sc.pythonExec,\n\u001b[32m   2832\u001b[39m                               sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\rdd.py:2816\u001b[39m, in \u001b[36m_prepare_for_python_RDD\u001b[39m\u001b[34m(sc, command)\u001b[39m\n\u001b[32m   2813\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prepare_for_python_RDD\u001b[39m(sc, command):\n\u001b[32m   2814\u001b[39m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[32m   2815\u001b[39m     ser = CloudPickleSerializer()\n\u001b[32m-> \u001b[39m\u001b[32m2816\u001b[39m     pickled_command = \u001b[43mser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2817\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) > sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[32m   2818\u001b[39m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n\u001b[32m   2819\u001b[39m         broadcast = sc.broadcast(pickled_command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ghoniem\\AppData\\Local\\anaconda3\\envs\\spark\\Lib\\site-packages\\pyspark\\serializers.py:447\u001b[39m, in \u001b[36mCloudPickleSerializer.dumps\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    445\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (e.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, emsg)\n\u001b[32m    446\u001b[39m print_exec(sys.stderr)\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m pickle.PicklingError(msg)\n",
      "\u001b[31mPicklingError\u001b[39m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "from datetime import date, datetime\n",
    "from pyspark import Row\n",
    "\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16fb6b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61eeb238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java version \"1.8.0_451\"\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_451-b10)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.451-b10, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "!java -version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecca262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
